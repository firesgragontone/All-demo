

软件：haproxy---主要是做负载均衡的7层，也可以做4层负载均衡

apache也可以做7层负载均衡，但是很麻烦。实际工作中没有人用；

负载均衡是通过OSI协议对应的；

7层负载均衡：用的7层http协议；

4层负载均衡：用的是tcp协议加端口号做的负载均衡；



## ha-proxy概述

ha-proxy是一款高性能的负载均衡软件。因为其专注于负载均衡这一些事情，因此与nginx比起来在负载均衡这件事情上做的更好，更专业。

### ha-proxy的特点

```text
ha-proxy 作为目前流行的负载均衡软件，必须有其出色的一面。下面介绍一下ha-proxy负载均衡软件的优点。

•支持tcp / http 两种协议层的负载均衡，使得其负载均衡功能非常丰富。

•支持多种负载均衡算法，尤其是在http模式时，有许多非常实在的负载均衡算法，适用各种需求。

•性能非常优秀，基于单进程处理模式（和Nginx类似）让其性能卓越。

•拥有一个功能出色的监控页面，实时了解系统的当前状况。

•功能强大的ACL支持，给用户极大的方便。

haproxy算法：

1.roundrobin

基于权重进行轮询,在服务器的处理时间保持均匀分布时,这是最平衡,最公平的算法.此算法是动态的,这表示其权重可以在运行时进行调整.

2.static-rr

基于权重进行轮询,与roundrobin类似,但是为静态方法,在运行时调整其服务器权重不会生效.不过,其在后端服务器连接数上没有限制

3.leastconn

新的连接请求被派发至具有最少连接数目的后端服务器.
```

## 实践准备

没有太多资源，用容器模拟实现

web 容器创建 [docker-compose 水平扩展 scale](https://link.zhihu.com/?target=https%3A//note.youdao.com/ynoteshare1/index.html%3Fid%3D279a6e553799e88e07eab6ea815348e8%26type%3Dnote)

```text
master 主节点服务器一台，正式使用建议多准备一台备用，node 代理服务器10台（用来进行web测试）,可以根据虚拟机配置自行调节；

配置如下：

# 创建文件
docker-compose.yml

haproxy.cfg
# docker-compose.yml
version: "3.6"

services:

  ha_proxy:
    container_name: ha_proxy
    image: haproxy:latest
    restart: always
    expose:
      - 80
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    networks:
      - traefik
    labels:
      - "traefik.enable=true"
      - "traefik.docker.network=traefik"
      - "traefik.http.routers.haproxy.entrypoints=http"
      - "traefik.http.routers.haproxy.rule=Host(`haproxy.halobug.cn`)"
      - "traefik.http.services.haproxy-backend.loadbalancer.server.scheme=http"
      - "traefik.http.services.haproxy-backend.loadbalancer.server.port=80"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"

networks:
  traefik:
    external: true
# haproxy.cfg

global
        log 127.0.0.1 local0
        log 127.0.0.1 local1 notice
defaults
        log global
        mode http
        option httplog
        option dontlognull
        timeout connect 5000ms
        timeout client 50000ms
        timeout server 50000ms
        stats uri /status
frontend balancer
        bind 0.0.0.0:80
        default_backend web_backends
backend web_backends
        balance roundrobin
        # 启动了 10个容器 进行web测试，
        server server1 local_2_web_nginx_1:80 check
        server server2 local_2_web_nginx_2:80 check
        server server3 local_2_web_nginx_3:80 check
        server server4 local_2_web_nginx_4:80 check
        server server5 local_2_web_nginx_5:80 check
        server server6 local_2_web_nginx_6:80 check
        server server7 local_2_web_nginx_7:80 check
        server server8 local_2_web_nginx_8:80 check
        server server9 local_2_web_nginx_9:80 check
        server server10 local_2_web_nginx_10:80 check
#启动

docker-compose down && docker-compose up -d

# 绑定hosts 

127.0.0.1 haproxy.halobug.cn
```

haproxy容器



![img](https://pic4.zhimg.com/80/v2-e5bfc88dbc3e383f61dfd53c96ac414b_720w.jpg)



web容器



![img](https://pic1.zhimg.com/80/v2-b2dee30e1a5ec011199ff3427ca935ac_720w.jpg)



浏览器访问 [http://haproxy.halobug.cn](https://link.zhihu.com/?target=http%3A//haproxy.halobug.cn/)，如下图。



![img](https://pic4.zhimg.com/80/v2-ac321fa6e221a21f7686361cb146792b_720w.jpg)



根据haproxy.cfg 配置 查看 haproxy 监控，[http://haproxy.halobug.cn/status](https://link.zhihu.com/?target=http%3A//haproxy.halobug.cn/status) ，如下图。



![img](https://pic2.zhimg.com/80/v2-34ee73c6f3bbcc9bf4bec8ac0fca848d_720w.jpg)



使用ab测试 查看分发效果

```text
# 执行后查看 status 监控
ab -n 20000 -c 200 http://haproxy.halobug.cn/
```

监控很清晰的能看见每台机器当前分发的请求量。



![img](https://pic2.zhimg.com/80/v2-10ca8f8a1c6d15b30b124ae9886ba775_720w.jpg)



试着停掉某一台机器

```text
docker stop local_2_web_nginx_8
```

可以看到server 8的机器状态down掉了，继续使用ab测试，如下图 server8 是down掉的，此时请求不会分发到这台机器,重新启动后恢复正常。



![img](https://pic3.zhimg.com/80/v2-b8195a6910dab8d51035b46ef8adb9d6_720w.jpg)



##### keepalived

详情请参考：[详细的详细的内容](http://outofmemory.cn/wiki/keepalived-configuration)
**1)keepalived是什么**
[keepalived](https://so.csdn.net/so/search?q=keepalived&spm=1001.2101.3001.7020)是集群管理中保证集群高可用的一个服务软件，其功能类似于heartbeat，用来防止单点故障。
**2)keepalived工作原理**
keepalived是以VRRP协议为实现基础的，VRRP全称Virtual Router Redundancy Protocol，即虚拟路由[冗余](https://so.csdn.net/so/search?q=冗余&spm=1001.2101.3001.7020)协议。

虚拟路由冗余协议，可以认为是实现[路由器](https://so.csdn.net/so/search?q=路由器&spm=1001.2101.3001.7020)高可用的协议，即将N台提供相同功能的路由器组成一个路由器组，这个组里面有一个master和多个backup，master上面有一个对外提供服务的vip（该路由器所在局域网内其他机器的默认路由为该vip），master会发组播，当backup收不到vrrp包时就认为master宕掉了，这时就需要根据VRRP的优先级来选举一个backup当master。这样的话就可以保证路由器的高可用了。

keepalived主要有三个模块，分别是core、check和vrrp。core模块为keepalived的核心，负责主进程的启动、维护以及全局配置文件的加载和解析。check负责健康检查，包括常见的各种检查方式。vrrp模块是来实现VRRP协议的。
**3)keepalived的配置文件**
keepalived只有一个配置文件keepalived.conf，里面主要包括以下几个配置区域，分别是global_defs、static_ipaddress、static_routes、vrrp_script、vrrp_instance和virtual_server。







# spEL表达式注入漏洞以及预防措施